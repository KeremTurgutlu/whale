{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "from fastai import *\n",
    "from fastai.vision import *\n",
    "from ipyexperiments import *\n",
    "from fastai.basic_train import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe use `loss(reduction=\"sum\")` and average before `real_step()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AccumulateOptimWrapper(OptimWrapper):\n",
    "    def step(self):          pass\n",
    "    def zero_grad(self):      pass\n",
    "    def real_step(self):      super().step()\n",
    "    def real_zero_grad(self): super().zero_grad()\n",
    "        \n",
    "def acc_create_opt(self, lr:Floats, wd:Floats=0.):\n",
    "        \"Create optimizer with `lr` learning rate and `wd` weight decay.\"\n",
    "        self.opt = AccumulateOptimWrapper.create(self.opt_func, lr, self.layer_groups,\n",
    "                                         wd=wd, true_wd=self.true_wd, bn_wd=self.bn_wd)\n",
    "        \n",
    "@dataclass\n",
    "class AccumulateStep(LearnerCallback):\n",
    "    \"\"\"\n",
    "    Does accumlated step every nth step by accumulating gradients\n",
    "    \"\"\"\n",
    "    def __init__(self, learn:Learner, n_step:int = 1):\n",
    "        super().__init__(learn)\n",
    "        self.n_step = n_step\n",
    " \n",
    "    def on_train_begin(self, **kwargs):\n",
    "        \"check if loss is reduction\"\n",
    "        if self.loss_func.reduction == \"mean\":\n",
    "             print(\"For better gradients consider 'reduction=sum'\")\n",
    "        \n",
    "    def on_epoch_begin(self, **kwargs):\n",
    "        \"init samples and batches, change optimizer\"\n",
    "        self.acc_samples = 0\n",
    "        self.acc_batches = 0\n",
    "        \n",
    "    def on_batch_begin(self, last_input, last_target, **kwargs):\n",
    "        \"accumulate samples and batches\"\n",
    "        self.acc_samples += last_input.shape[0]\n",
    "        self.acc_batches += 1\n",
    "#         print(f\"At batch {self.acc_batches}\")\n",
    "        \n",
    "    def on_backward_end(self, **kwargs):\n",
    "        \"step if number of desired batches accumulated, reset samples\"\n",
    "        if (self.acc_batches % self.n_step) == 0:\n",
    "            for p in (self.learn.model.parameters()):\n",
    "                if p.requires_grad: p.grad.div_(self.acc_samples)\n",
    "    \n",
    "#             print(f\"Stepping at batch: {self.acc_batches}\")\n",
    "            self.learn.opt.real_step()\n",
    "            self.learn.opt.real_zero_grad()\n",
    "            self.acc_samples = 0\n",
    "    \n",
    "    def on_epoch_end(self, **kwargs):\n",
    "        \"step the rest of the accumulated grads\"\n",
    "        self.learn.opt.real_step()\n",
    "        self.learn.opt.real_zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST batch size = 4, no accumulation, 4 epochs\n",
    "\n",
    "`effective batch size = 4`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../dev\")\n",
    "from data_utils import seed_everything\n",
    "from fastai.basic_train import Learner; original_create_opt = Learner.create_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original optimwrapper\n",
    "Learner.create_opt = original_create_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed everything for reproducibility\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = untar_data(URLs.MNIST_SAMPLE)\n",
    "data = ImageDataBunch.from_folder(path, bs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = create_cnn(data, models.resnet18, metrics=accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.loss_func = CrossEntropyFlat(reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Total time: 03:25 <p><table style='width:300px; margin-bottom:10px'>\n",
       "  <tr>\n",
       "    <th>epoch</th>\n",
       "    <th>train_loss</th>\n",
       "    <th>valid_loss</th>\n",
       "    <th>accuracy</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>1</th>\n",
       "    <th>0.396730</th>\n",
       "    <th>0.495012</th>\n",
       "    <th>0.829735</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>2</th>\n",
       "    <th>0.329213</th>\n",
       "    <th>0.482487</th>\n",
       "    <th>0.856232</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>3</th>\n",
       "    <th>0.323470</th>\n",
       "    <th>0.328319</th>\n",
       "    <th>0.876349</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>4</th>\n",
       "    <th>0.319317</th>\n",
       "    <th>9.076464</th>\n",
       "    <th>0.852797</th>\n",
       "  </tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST batch size=4, accumulate every n_step=8, 4 epochs\n",
    "\n",
    "`effective batch size = 32 (bs x n_step)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed everything for reproducibility\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# monkey patch \n",
    "Learner.create_opt = acc_create_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = untar_data(URLs.MNIST_SAMPLE)\n",
    "data = ImageDataBunch.from_folder(path, bs=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = create_cnn(data, models.resnet18, metrics=accuracy,\n",
    "                   callback_fns=[partial(AccumulateStep, n_step=8)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[fastai.basic_train.Recorder,\n",
       " functools.partial(<class '__main__.AccumulateStep'>, n_step=8)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.callback_fns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.loss_func = CrossEntropyFlat(reduction='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Total time: 00:21 <p><table style='width:300px; margin-bottom:10px'>\n",
       "  <tr>\n",
       "    <th>epoch</th>\n",
       "    <th>train_loss</th>\n",
       "    <th>valid_loss</th>\n",
       "    <th>accuracy</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>1</th>\n",
       "    <th>5.950532</th>\n",
       "    <th>4.001670</th>\n",
       "    <th>0.958783</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>2</th>\n",
       "    <th>5.227745</th>\n",
       "    <th>3.383670</th>\n",
       "    <th>0.962218</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>3</th>\n",
       "    <th>5.597690</th>\n",
       "    <th>4.019064</th>\n",
       "    <th>0.957311</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>4</th>\n",
       "    <th>5.486883</th>\n",
       "    <th>3.625966</th>\n",
       "    <th>0.963199</th>\n",
       "  </tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
