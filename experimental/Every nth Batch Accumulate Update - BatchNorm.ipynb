{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "from fastai import *\n",
    "from fastai.vision import *\n",
    "from ipyexperiments import *\n",
    "from fastai.basic_train import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../dev\")\n",
    "from data_utils import seed_everything\n",
    "from fastai.train import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bn_layers(m:nn.Module)->None:\n",
    "    \"Set bn layers in eval mode for all recursive children of `m`.\"\n",
    "    for l in m.children():\n",
    "        if isinstance(l, bn_types):\n",
    "            return l\n",
    "        find_active_bn(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AccumulateBatchNorm(nn.Module):\n",
    "    \n",
    "    def __init__(self, bn_class, num_features, eps=1e-5, momentum=0.1, affine=True,\n",
    "                 track_running_stats=True):\n",
    "        super().__init__()\n",
    "        self.num_features = num_features\n",
    "        self.track_running_stats,self.momentum = track_running_stats,momentum\n",
    "        self.bn = bn_class(num_features, eps=eps, momentum=momentum, affine=affine,\n",
    "                 track_running_stats=track_running_stats)\n",
    "        self.running_mean,self.running_square,self.iterations = None,None,None\n",
    "    \n",
    "    def reset_running_stats(self):\n",
    "        self.running_mean,self.running_square,self.iterations = None,None,None\n",
    "        self.bn.reset_running_stats()\n",
    "    \n",
    "    def update_stats(self):\n",
    "        if self.training and self.track_running_stats:\n",
    "            self.bn.num_batches_tracked += 1\n",
    "            eaf = 1.0 / float(self.bn.num_batches_tracked) if self.bn.momentum is None else self.bn.momentum\n",
    "            self.bn.running_mean = self.bn.running_mean * (1-eaf) + self.running_mean * eaf / self.iterations\n",
    "            var = self.running_square/self.iterations - (self.running_mean/self.iterations).pow(2)\n",
    "            self.bn.running_var  = self.bn.running_var  * (1-eaf) + var  * eaf\n",
    "            self.running_mean,self.running_square,self.iterations = None,None,None\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        self.bn.reset_parameters()\n",
    "        \n",
    "    def forward(self, input):\n",
    "        self.bn._check_input_dim(input)\n",
    "        if self.track_running_stats:\n",
    "            if self.iterations is None:\n",
    "                self.running_mean   = self.bn.weight.new_zeros(self.num_features)\n",
    "                self.running_square = self.bn.weight.new_zeros(self.num_features)\n",
    "                self.iterations   = 0\n",
    "            self.running_mean += input.view(input.size(0), input.size(1), -1).mean(2).sum(0)\n",
    "            self.running_square += input.view(input.size(0), input.size(1), -1).pow(2).mean(2).sum(0)\n",
    "            self.iterations += input.size(0)\n",
    "        return torch.batch_norm(input, self.bn.weight, self.bn.bias, self.bn.running_mean, self.bn.running_var, \n",
    "            False, 0., self.bn.eps, torch.backends.cudnn.enabled)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AccumulateStepper(LearnerCallback):\n",
    "    \"Does accumlated step every nth step by accumulating gradients\"\n",
    "\n",
    "    def __init__(self, learn:Learner, n_step:int = 1, drop_last:bool = False):\n",
    "        super().__init__(learn)\n",
    "        self.n_step,self.drop_last = n_step,drop_last\n",
    "        # wrap all BN layers\n",
    "        \n",
    " \n",
    "    def on_train_begin(self, **kwargs):\n",
    "        \"check if loss is reduction\"\n",
    "        if hasattr(self.loss_func, \"reduction\") and (self.loss_func.reduction != \"sum\"):\n",
    "             warn(\"For better gradients consider 'reduction=sum'\")\n",
    "        \n",
    "    def on_epoch_begin(self, **kwargs):\n",
    "        \"init samples and batches, change optimizer\"\n",
    "        self.acc_samples, self.acc_batches = 0., 0. \n",
    "        \n",
    "    def on_batch_begin(self, last_input, last_target, **kwargs):\n",
    "        \"accumulate samples and batches\"\n",
    "        self.acc_samples += last_input.shape[0]\n",
    "        self.acc_batches += 1\n",
    "        \n",
    "    def on_backward_end(self, **kwargs):\n",
    "        \"accumulated step and reset samples, True will result in no stepping\"\n",
    "        if (self.acc_batches % self.n_step) == 0:\n",
    "            for p in (self.learn.model.parameters()):\n",
    "                if p.requires_grad: p.grad.div_(self.acc_samples)\n",
    "            self.acc_samples = 0\n",
    "        else: return True\n",
    "    \n",
    "    def on_step_end(self, **kwargs):\n",
    "        \"zero gradients after stepping, True will result in no zeroing\"\n",
    "        return (self.acc_batches % self.n_step) != 0\n",
    "    \n",
    "    def on_epoch_end(self, **kwargs):\n",
    "        \"step the rest of the accumulated grads if not perfectly divisible\"\n",
    "        for p in (self.learn.model.parameters()):\n",
    "                if p.requires_grad: p.grad.div_(self.acc_samples)\n",
    "        if not self.drop_last: self.learn.opt.step()\n",
    "        self.learn.opt.zero_grad()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe use `loss(reduction=\"sum\")` and average before `real_step()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST batch size = 32, no accumulation\n",
    "\n",
    "`effective batch size = 32`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed everything for reproducibility\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = untar_data(URLs.MNIST_SAMPLE)\n",
    "data = ImageDataBunch.from_folder(path, bs=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = create_cnn(data, models.resnet18, metrics=accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[find_bn_layers(learn.model)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.loss_func = CrossEntropyFlat(reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Total time: 00:07 <p><table style='width:300px; margin-bottom:10px'>\n",
       "  <tr>\n",
       "    <th>epoch</th>\n",
       "    <th>train_loss</th>\n",
       "    <th>valid_loss</th>\n",
       "    <th>accuracy</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>1</th>\n",
       "    <th>0.141535</th>\n",
       "    <th>0.079934</th>\n",
       "    <th>0.972031</th>\n",
       "  </tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST batch size=4, accumulate every n_step=8, Default\n",
    "\n",
    "`effective batch size = 32 (bs x n_step)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed everything for reproducibility\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = untar_data(URLs.MNIST_SAMPLE)\n",
    "data = ImageDataBunch.from_folder(path, bs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze_to(-1) except for BN layers\n",
    "learn = create_cnn(data, models.resnet18, metrics=accuracy,\n",
    "                   callback_fns=[partial(AccumulateStepper, n_step=16)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.loss_func = CrossEntropyFlat(reduction='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Total time: 01:13 <p><table style='width:300px; margin-bottom:10px'>\n",
       "  <tr>\n",
       "    <th>epoch</th>\n",
       "    <th>train_loss</th>\n",
       "    <th>valid_loss</th>\n",
       "    <th>accuracy</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>1</th>\n",
       "    <th>1.153869</th>\n",
       "    <th>2.707523</th>\n",
       "    <th>0.507851</th>\n",
       "  </tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST batch size=4, accumulate every n_step=8, BNFreeze\n",
    "\n",
    "`effective batch size = 32 (bs x n_step)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.train import BnFreeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BnFreeze(LearnerCallback):\n",
    "    \"Freeze moving average statistics in all non-trainable batchnorm layers.\"\n",
    "    def on_train_begin(self, **kwargs:Any)->None:\n",
    "        \"Put bn layers in eval mode just after `model.train()`.\"\n",
    "        set_bn_eval(self.learn.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed everything for reproducibility\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = untar_data(URLs.MNIST_SAMPLE)\n",
    "data = ImageDataBunch.from_folder(path, bs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze_to(-1) including BN layers\n",
    "learn = create_cnn(data, models.resnet18, metrics=accuracy,\n",
    "                   callback_fns=[partial(AccumulateStepper, n_step=16), BnFreeze])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze bn layers\n",
    "for g in learn.layer_groups[:-1]:\n",
    "    for l in g:\n",
    "        if isinstance(l, bn_types): requires_grad(l, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"
     ]
    }
   ],
   "source": [
    "find_active_bn(learn.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.loss_func = CrossEntropyFlat(reduction='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Total time: 00:51 <p><table style='width:300px; margin-bottom:10px'>\n",
       "  <tr>\n",
       "    <th>epoch</th>\n",
       "    <th>train_loss</th>\n",
       "    <th>valid_loss</th>\n",
       "    <th>accuracy</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>1</th>\n",
       "    <th>1.140334</th>\n",
       "    <th>2.693619</th>\n",
       "    <th>0.601570</th>\n",
       "  </tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST batch size=4, accumulate every n_step=8, BNFreeze + More momentum\n",
    "\n",
    "`effective batch size = 32 (bs x n_step)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.train import BnFreeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BnFreeze(LearnerCallback):\n",
    "    \"Freeze moving average statistics in all non-trainable batchnorm layers.\"\n",
    "    def on_train_begin(self, **kwargs:Any)->None:\n",
    "        \"Put bn layers in eval mode just after `model.train()`.\"\n",
    "        set_bn_eval(self.learn.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed everything for reproducibility\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = untar_data(URLs.MNIST_SAMPLE)\n",
    "data = ImageDataBunch.from_folder(path, bs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze_to(-1) including BN layers\n",
    "learn = create_cnn(data, models.resnet18, metrics=accuracy,\n",
    "                   callback_fns=[partial(AccumulateStepper, n_step=16), BnFreeze])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # freeze bn layers\n",
    "# for g in learn.layer_groups[:-1]:\n",
    "#     for l in g:\n",
    "#         if isinstance(l, bn_types): requires_grad(l, False)\n",
    "for g in learn.layer_groups:\n",
    "    for l in g:\n",
    "        if isinstance(l, bn_types): l.momentum = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BatchNorm2d(64, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
      "BatchNorm2d(64, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
      "BatchNorm2d(64, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
      "BatchNorm2d(64, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
      "BatchNorm2d(64, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
      "BatchNorm2d(128, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
      "BatchNorm2d(128, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
      "BatchNorm2d(128, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
      "BatchNorm2d(128, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
      "BatchNorm2d(128, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
      "BatchNorm2d(256, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
      "BatchNorm2d(256, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
      "BatchNorm2d(256, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
      "BatchNorm2d(256, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
      "BatchNorm2d(256, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
      "BatchNorm2d(512, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
      "BatchNorm2d(512, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
      "BatchNorm2d(512, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
      "BatchNorm2d(512, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
      "BatchNorm2d(512, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
      "BatchNorm1d(1024, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
      "BatchNorm1d(512, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n"
     ]
    }
   ],
   "source": [
    "find_active_bn(learn.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.loss_func = CrossEntropyFlat(reduction='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Total time: 01:11 <p><table style='width:300px; margin-bottom:10px'>\n",
       "  <tr>\n",
       "    <th>epoch</th>\n",
       "    <th>train_loss</th>\n",
       "    <th>valid_loss</th>\n",
       "    <th>accuracy</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>1</th>\n",
       "    <th>1.153869</th>\n",
       "    <th>411.230682</th>\n",
       "    <th>0.479392</th>\n",
       "  </tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST batch size=4, accumulate every n_step=8, BNFreeze + Custom Running\n",
    "\n",
    "`effective batch size = 32 (bs x n_step)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed everything for reproducibility\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = untar_data(URLs.MNIST_SAMPLE)\n",
    "data = ImageDataBunch.from_folder(path, bs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = create_cnn(data, models.resnet18, metrics=accuracy,\n",
    "                   callback_fns=[partial(AccumulateStepper, n_step=16)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.loss_func = CrossEntropyFlat(reduction='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Total time: 04:41 <p><table style='width:300px; margin-bottom:10px'>\n",
       "  <tr>\n",
       "    <th>epoch</th>\n",
       "    <th>train_loss</th>\n",
       "    <th>valid_loss</th>\n",
       "    <th>accuracy</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>1</th>\n",
       "    <th>1.543319</th>\n",
       "    <th>1.134782</th>\n",
       "    <th>0.743867</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>2</th>\n",
       "    <th>1.379197</th>\n",
       "    <th>1.387173</th>\n",
       "    <th>0.564769</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>3</th>\n",
       "    <th>1.277315</th>\n",
       "    <th>1.233230</th>\n",
       "    <th>0.696762</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>4</th>\n",
       "    <th>1.208992</th>\n",
       "    <th>1.130871</th>\n",
       "    <th>0.759078</th>\n",
       "  </tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BatchNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3367,  0.1288,  0.2345,  0.2303, -1.1229],\n",
       "        [-0.1863,  2.2082, -0.6380,  0.4617,  0.2674]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn((2, 5));x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "bn = nn.BatchNorm1d(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0., 0., 0., 0., 0.]),\n",
       " tensor([1., 1., 1., 1., 1.]),\n",
       " Parameter containing:\n",
       " tensor([0.2696, 0.4414, 0.2969, 0.8317, 0.1053], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0., 0., 0., 0., 0.], requires_grad=True),\n",
       " 1e-05)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mom = bn.momentum, \n",
    "mean = bn.running_mean\n",
    "var = bn.running_var\n",
    "w = bn.weight\n",
    "b = bn.bias\n",
    "eps = bn.eps\n",
    "\n",
    "mean, var, w, b, eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2696, -0.4414,  0.2969, -0.8314, -0.1053],\n",
       "        [-0.2696,  0.4414, -0.2969,  0.8314,  0.1053]],\n",
       "       grad_fn=<NativeBatchNormBackward>)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.0352,  0.5475, -0.0945,  0.1621, -0.2004]),\n",
       " tensor([0.5955, 1.5444, 0.7098, 0.5440, 0.9842]),\n",
       " Parameter containing:\n",
       " tensor([0.2696, 0.4414, 0.2969, 0.8317, 0.1053], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0., 0., 0., 0., 0.], requires_grad=True),\n",
       " 1e-05)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mom = bn.momentum, \n",
    "mean = bn.running_mean\n",
    "var = bn.running_var\n",
    "w = bn.weight\n",
    "b = bn.bias\n",
    "eps = bn.eps\n",
    "\n",
    "mean, var, w, b, eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = learn.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (2): ReLU(inplace)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model[0][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "bn = model[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.1, tensor([ 2.7681e-03, -2.5769e-02,  2.1254e-07, -8.4605e-02,  2.1121e-08,\n",
       "          4.9691e-04, -2.2408e-02, -1.1582e-07, -4.8239e-03,  2.7507e-07,\n",
       "          3.9582e-02,  3.1994e-02, -3.7490e-02, -1.3716e-06,  6.6002e-03,\n",
       "          4.3782e-03,  6.4797e-02,  1.1176e-01,  3.6002e-02, -7.5075e-02,\n",
       "         -3.8240e-02,  8.4358e-02, -5.2287e-02, -1.1799e-02,  1.3019e-03,\n",
       "          3.2172e-02, -1.7784e-02, -9.1009e-02,  1.1319e-01, -4.1632e-02,\n",
       "          8.7302e-03,  2.9693e-02, -7.0502e-02, -3.4847e-03,  1.0977e-01,\n",
       "         -1.7341e-03, -5.9423e-08,  2.9330e-02, -7.8553e-09,  6.7320e-03,\n",
       "         -3.7100e-03,  1.6028e-02, -2.7883e-02,  2.6593e-02,  2.8475e-02,\n",
       "         -1.2735e-01,  4.4617e-02,  2.6329e-02,  2.1454e-08, -1.7045e-02,\n",
       "         -3.5617e-03, -4.5841e-02,  6.3876e-02,  1.5220e-02, -3.8511e-02,\n",
       "         -1.6428e-02, -1.6569e-02,  5.6057e-02, -8.0306e-02, -2.6646e-03,\n",
       "         -4.1718e-02,  1.2611e-01, -4.9237e-02, -1.3261e-02], device='cuda:0'), tensor([1.0169e+00, 3.7167e+00, 5.8133e-11, 3.2825e+00, 1.7107e-13, 6.5823e-01,\n",
       "         4.3701e+00, 6.6005e-12, 9.1552e-01, 1.9318e-09, 4.1256e+00, 2.7440e+00,\n",
       "         2.8391e+00, 4.7966e-08, 1.1072e+01, 5.0075e-01, 2.2313e+00, 4.8257e+00,\n",
       "         2.6986e+00, 9.3700e+00, 3.7339e+00, 5.4843e+00, 5.7127e+00, 4.4544e-01,\n",
       "         4.3628e-01, 7.1563e+00, 1.3718e+01, 5.2512e+00, 6.8174e+00, 1.6724e+00,\n",
       "         1.6534e+00, 1.2325e+00, 4.9076e+00, 3.0731e+00, 4.2384e+00, 4.9936e+00,\n",
       "         1.4465e-12, 1.5212e+00, 1.0352e-13, 3.5134e-01, 1.7025e-01, 1.4205e+00,\n",
       "         1.9085e+00, 2.1512e+00, 2.6608e+00, 4.8444e+00, 1.9297e+00, 1.4999e+00,\n",
       "         2.9481e-13, 1.5306e+00, 3.6503e-01, 2.9376e+00, 5.4664e+00, 7.0792e-01,\n",
       "         3.3315e+00, 7.7180e-01, 2.4068e+00, 6.5214e+00, 4.1263e+00, 1.0506e+00,\n",
       "         2.9530e+00, 1.1366e+01, 4.7690e+00, 1.6559e+00], device='cuda:0'), Parameter containing:\n",
       " tensor([ 2.3487e-01,  2.6626e-01, -5.1096e-08,  5.1870e-01,  3.4404e-09,\n",
       "          2.2239e-01,  4.2289e-01,  1.3153e-07,  2.5093e-01,  1.5152e-06,\n",
       "          3.1687e-01,  2.5049e-01,  3.7893e-01,  1.0862e-05,  2.7526e-01,\n",
       "          2.3674e-01,  2.4202e-01,  3.9531e-01,  4.6935e-01,  2.9090e-01,\n",
       "          2.7268e-01,  2.7803e-01,  2.9069e-01,  2.0693e-01,  2.5899e-01,\n",
       "          2.7871e-01,  2.9115e-01,  3.1601e-01,  3.8889e-01,  3.0411e-01,\n",
       "          2.6776e-01,  2.1093e-01,  2.8708e-01,  3.3243e-01,  4.2673e-01,\n",
       "          3.7326e-01,  7.4804e-08,  1.9068e-01,  1.4740e-08,  2.2303e-01,\n",
       "          1.7908e-01,  2.4860e-01,  2.7400e-01,  2.5923e-01,  2.9420e-01,\n",
       "          2.9924e-01,  2.2369e-01,  2.6280e-01,  2.2001e-08,  2.6610e-01,\n",
       "          2.2089e-01,  2.8429e-01,  3.3072e-01,  2.2681e-01,  3.6538e-01,\n",
       "          2.1230e-01,  2.3965e-01,  2.4950e-01,  5.2583e-01,  2.4825e-01,\n",
       "          2.9565e-01,  2.5878e-01,  4.8326e-01,  2.6670e-01], device='cuda:0',\n",
       "        requires_grad=True), Parameter containing:\n",
       " tensor([ 2.3072e-01,  2.5382e-01, -1.0543e-06, -6.6439e-01, -1.6571e-08,\n",
       "          1.6152e-01,  4.5450e-01, -4.3020e-07,  3.0051e-01, -8.0052e-06,\n",
       "          3.4942e-01,  3.1148e-01, -2.4953e-01, -3.4749e-05,  1.0773e-01,\n",
       "          2.1897e-01,  3.8141e-01, -5.2988e-01, -6.2864e-01,  5.7140e-01,\n",
       "          2.9985e-01,  5.8430e-01,  4.8202e-01,  3.2853e-01,  1.9672e-01,\n",
       "          1.9496e-01,  1.5215e-01,  8.5522e-02,  5.1314e-01,  1.5237e-02,\n",
       "          1.6644e-01,  3.3239e-01,  2.4921e-01,  4.4337e-01, -2.8017e-01,\n",
       "         -2.0385e-02, -2.4507e-07,  3.2134e-01, -4.9152e-08,  2.3777e-01,\n",
       "          2.3291e-01,  3.1527e-01,  4.2776e-01,  2.9313e-01,  2.6379e-01,\n",
       "          6.7598e-01,  4.2910e-01,  3.4566e-01, -8.6909e-08,  2.4729e-01,\n",
       "          3.0316e-01,  6.1577e-01,  3.9835e-01,  3.3207e-01, -4.1219e-01,\n",
       "          3.7807e-01,  1.7895e-01,  2.5748e-01, -4.4908e-01,  2.1306e-01,\n",
       "          5.6934e-01,  5.7274e-01, -4.0238e-01,  2.3406e-01], device='cuda:0',\n",
       "        requires_grad=True), 1e-05)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn.momentum, bn.running_mean, bn.running_var, bn.weight, bn.bias, bn.eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyBatchNorm(nn.Module):\n",
    "    def __init__(self, num_features, eps=1e-05, momentum=0.1, affine=True):\n",
    "        super(MyBatchNorm, self).__init__()\n",
    "        self.bn = nn.BatchNorm1d(num_features,\n",
    "                                 eps=eps,\n",
    "                                 momentum=momentum,\n",
    "                                 affine=affine)\n",
    "\n",
    "    def forward(self, x):        \n",
    "        x = self.bn(x)\n",
    "        mu = self.bn.running_mean\n",
    "        var = self.bn.running_var\n",
    "        gamma = self.bn.weight\n",
    "        beta = self.bn.bias\n",
    "        eps = self.bn.eps    \n",
    "        k = gamma.data / torch.sqrt(var + eps)\n",
    "        x.data = k * x.data + beta.data\n",
    "        return x\n",
    "\n",
    "mybn = MyBatchNorm(10)\n",
    "x = Variable(torch.randn(16, 10))\n",
    "x_ = mybn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
